{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fully-Connected Feed-Forward NN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yeHb5uBFiDjh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "259453c4-c10e-48ba-a746-56307cae2e4b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-846021d4c7e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: generic_type: cannot initialize type \"TensorProtoDataType\": an object with that name is already defined"
          ]
        }
      ],
      "source": [
        "import torch, torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from torchsummary import summary\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "9rVWPOS6RS_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548aae6d-69d8-4f41-eddb-2183ddc97b60"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/MCTE7106FinalProject/KaggleDataset/newdataset\""
      ],
      "metadata": {
        "id": "iNMFlz7kRWS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d140a29a-b72c-4115-a736-cd59f3890f45"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test  Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Applying Transforms to the Data\n",
        "image_transforms = { \n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(size=(32,32)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize(size=(32,32)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size=(32,32)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "gRcatYsjS1b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "e3116118-9357-4d1c-b05b-3d3883cc163b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1598731a87d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Applying Transforms to the Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m image_transforms = { \n\u001b[0;32m----> 4\u001b[0;31m     'train': transforms.Compose([\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomRotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpES3y6LNN_k"
      },
      "source": [
        "\n",
        "dataset = '/content/drive/My Drive/MCTE7106FinalProject/KaggleDataset/newdataset'\n",
        "\n",
        "train_directory = os.path.join(dataset, 'Training')\n",
        "valid_directory = os.path.join(dataset, 'Test')\n",
        "test_directory = os.path.join(dataset, 'Test')\n",
        "\n",
        "# Batch size\n",
        "batchSize = 32\n",
        "\n",
        "# Number of classes \n",
        "num_classes = len(os.listdir(valid_directory))\n",
        "print(num_classes)\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
        "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid']),\n",
        "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
        "}\n",
        "\n",
        "# Get a mapping of the indices to the class names in order to see the output classes of the test images.\n",
        "idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
        "print(idx_to_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train']"
      ],
      "metadata": {
        "id": "bJGyN5mFVBns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
        "train_data_size = len(data['train'])\n",
        "valid_data_size = len(data['valid'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "# Create iterators for the Data loaded using DataLoader module\n",
        "train_data_loader = DataLoader(data['train'], batch_size=batchSize, shuffle=True)\n",
        "valid_data_loader = DataLoader(data['valid'], batch_size=batchSize, shuffle=True)\n",
        "test_data_loader = DataLoader(data['test'], batch_size=batchSize, shuffle=True)"
      ],
      "metadata": {
        "id": "bZpPl_NEVHYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_size, valid_data_size, test_data_size"
      ],
      "metadata": {
        "id": "L7dIcXttVUzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 32*32*3 "
      ],
      "metadata": {
        "id": "hGwkLwyDVXNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ourModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, 2000)\n",
        "        # hidden layers\n",
        "        self.linear2 = nn.Linear(2000, 200)\n",
        "        self.linear3 = nn.Linear(200, 100)\n",
        "        # output layer\n",
        "        self.linear4 = nn.Linear(100, num_classes)\n",
        "        # Define proportion or neurons to dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        # Flatten images into vectors\n",
        "        out = xb.view(xb.size(0), -1)\n",
        "        # Apply layers & activation functions\n",
        "        out = self.linear1(out)\n",
        "        # Apply activation function. Changes in activation functions are applied here.\n",
        "        out = torch.sigmoid(out)\n",
        "        # Apply dropout\n",
        "        #out = self.dropout(out)\n",
        "        # Get intermediate outputs using hidden layer 2\n",
        "        out = self.linear2(out)\n",
        "        # Apply activation function.  Changes in activation functions are applied here.\n",
        "        out = torch.sigmoid(out)\n",
        "        # Apply dropout\n",
        "        #out = self.dropout(out)\n",
        "        # Get predictions using output layer\n",
        "        out = self.linear3(out)\n",
        "        # Apply activation function.  Changes in activation functions are applied here.\n",
        "        out = torch.sigmoid(out)\n",
        "        # Apply dropout\n",
        "        out = self.dropout(out)\n",
        "        # Get predictions using output layer\n",
        "        out = self.linear4(out)\n",
        "        # Apply activation function.  Changes in activation functions are applied here.\n",
        "        out = torch.sigmoid(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "LtEqMgkEWFKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ourModel(input_size, num_classes)\n",
        "\n",
        "model = model.to('cuda:0')"
      ],
      "metadata": {
        "id": "oNzuwQJxWmAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "JNctsVdcWrjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Optimizer and Loss Function\n",
        "learning_rate = 0.001 # 0.0000001\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "nsjZqt-NW_VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_valid_loss = valid_loss/valid_data_size \n",
        "        avg_valid_acc = valid_acc/valid_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
        "            \n",
        "    return model, history"
      ],
      "metadata": {
        "id": "WMt_rYTWXAmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "xuzagOsXXEnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "9PUbGRibXGx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for 25 epochs\n",
        "num_epochs = 25\n",
        "trained_model, history = train_and_validate(model, loss_func, optimizer, num_epochs)\n"
      ],
      "metadata": {
        "id": "IpSuqPidXIQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,1)\n",
        "plt.savefig(dataset+'_loss_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1FapdlPIXN0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "plt.savefig(dataset+'_accuracy_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "leCJ8KKsvN04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTestSetAccuracy(model, loss_criterion):\n",
        "    '''\n",
        "    Function to compute the accuracy on the test set\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "    '''\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    # Validation - No gradient tracking needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Validation loop\n",
        "        for j, (inputs, labels) in enumerate(test_data_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "\n",
        "            # Compute the total loss for the batch and add it to valid_loss\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "            # Compute total accuracy in the whole batch and add to valid_acc\n",
        "            test_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "            print(\"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "\n",
        "    # Find average test loss and test accuracy\n",
        "    avg_test_loss = test_loss/test_data_size \n",
        "    avg_test_acc = test_acc/test_data_size\n",
        "\n",
        "    print(\"Test accuracy : \" + str(avg_test_acc))\n"
      ],
      "metadata": {
        "id": "bbS-unb-vRBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, test_image_name):\n",
        "    '''\n",
        "    Function to predict the class of a single test image\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param test_image_name: Test image\n",
        "\n",
        "    '''\n",
        "    \n",
        "    transform = image_transforms['test']\n",
        "\n",
        "    test_image = Image.open(test_image_name)\n",
        "    plt.imshow(test_image)\n",
        "    \n",
        "    test_image_tensor = transform(test_image)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 32, 32).cuda()\n",
        "    else:\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 32, 32)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Model outputs log probabilities\n",
        "        out = model(test_image_tensor)\n",
        "        ps = torch.exp(out)\n",
        "        topk, topclass = ps.topk(3, dim=1)\n",
        "        for i in range(3):\n",
        "            print(\"Prediction\", i+1, \":\", idx_to_class[topclass.cpu().numpy()[0][i]], \", Score: \", topk.cpu().numpy()[0][i])\n"
      ],
      "metadata": {
        "id": "3i8xhQtPvTY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test a particular model on a test image\n",
        "\n",
        "dataset = '/content/drive/My Drive/MCTE7106FinalProject/KaggleDataset/newdataset/Test'\n",
        "model = torch.load('/content/drive/My Drive/MCTE7106FinalProject/KaggleDataset/newdataset_model_24.pt')\n",
        "predict(model, dataset+'/Banana/58_100.jpg')  "
      ],
      "metadata": {
        "id": "DIQ4cPwr7DGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data from folders\n",
        "computeTestSetAccuracy(model, loss_func)"
      ],
      "metadata": {
        "id": "dzV0UTHL7D2z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}